---
title: "PML Peer-graded Assignment"
author: "Roslen Anacleto"
date: "4/18/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load the required libraries and seed the random number generator

```{r initialize, results='hide', warning=FALSE}
# Suppress package startup messages
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(rattle))

# Make the results reproducible.
set.seed(315)
```

## Building the model

The source of training and testing data was http://groupware.les.inf.puc-rio.br/har. Previewing the downloaded data revealed that missing values were either represented as null strings or explicitly labeled as "NA". 

### Retrieving the data

```{r data_loading}
# Load the training and testing data as prescribed in the instuctions.
training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                     header = T, na.strings = c("", "NA"))
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
                    header = T, na.strings = c("", "NA"))
```


### Data quality check

Got a sense of the data structure by running commands such as:

* `dim(training)` --- showed 19,622 observations and 160 variables
* `dim(testing)` --- showed 20 observations and 160 variables
* `colnames(training)` --- showed the names of 160 variables
* `colSums(is.na(training)) %>% sort() %>% unique()` --- showed c(0, 19216)
* `colSums(is.na(testing)) %>% sort() %>% unique()` --- showed c(0, 20)

The results of `colSums()` showed that there were variables with many missing values. These were removed.

```{r remove_NA}
# Retain only those variables with no missing values; store in new variables.
tr <- training[, which(colSums(is.na(training)) == 0)]
ts <- testing[, which(colSums(is.na(testing)) == 0)]

# Exclude the first seven variables as predictors. They appear to be only descriptor variables.
tr <- tr[, -c(1:7)]
ts <- ts[, -c(1:7)]
```

### Preprocess

Do preprocessing to remove near-zero variance variables and colinearity.

```{r preprocess}
# Preprocess the datasets to remove near-zero variance variables and collinear
# variables.
prep <- preProcess(tr[, -53], method = c("nzv", "corr"))
tr <- predict(prep, tr)
ts <- predict(prep, ts)
# Note: No need to scale/center the data since classification trees, stochastic gradient boosting, and random forest methods will be used as prediction models.
```

### Data splitting

Split the `tr` (training) data into a training and validation subsets to estimate out-of-sample errors (primarily) and other parameters prior to doing predictions on the testing data.

```{r data_splitting}
# Split the training data to be able to estimate the out-of-sample error. A 70-30 partition scheme as recommended in the lecture.
idx <- createDataPartition(tr$classe, p = 0.70, list = F)

# Partition the training data superset into training and validation subsets.
tr.training <- tr[idx, ]
tr.validation <- tr[-idx, ]
```

## Resampling using cross-validation

```{r cross_validation}
# Decide on a training control strategy. Choose a relatively low number to
# prevent overfitting; use the value recommended from the lecture.
fit_control <- trainControl(method = "cv", number = 5)
```

## Training three prediction models

Compare three prediction models and choose the best one based on the training and validation accuracies.

```{r training}
# Do classification trees modeling
fit_rpart <- train(classe ~ ., data = tr.training, method = "rpart",
                   trControl = fit_control)
print(fit_rpart)

# Try stochastic gradient boosting
fit_gbm <- train(classe ~ ., data = tr.training,
                 method = "gbm",
                 trControl = fit_control,
                 verbose = F)
print(fit_gbm)

# Try random forest
fit_rf <- train(classe ~ ., data = tr.training,
                method = "rf",
                trControl = fit_control)
print(fit_rf)
```

Results show that classfication tree (RPART) had an accuracy of ~57%, stochastic gradient boosting (GBM) ~96%, and random forest (RF) ~99%. RF, therefore, shows the highest prediction accuracy rate on the training data.

## Plotting the predictive models

```{r plotting}
# Then, plot the final models
fancyRpartPlot(fit_rpart$finalModel, main = "Classification Tree")
plot(fit_gbm, main = "Stochastic Gradient Boosting")
plot(fit_rf, main = "Random Forest")
```

## Predictions on the validation data

```{r predictions_validation}
# Use the respective fitted models to prediction the classe values in the validation set.
predict_rpart <- predict(fit_rpart, tr.validation)
predict_gbm <- predict(fit_gbm, tr.validation)
predict_rf <- predict(fit_rf, tr.validation)

# Show confusion matrices of the three prediction models
confusionMatrix(tr.validation$classe, predict_rpart)
confusionMatrix(tr.validation$classe, predict_gbm)
confusionMatrix(tr.validation$classe, predict_rf)
```

### Out-of-sample error

Based on the confusion matrices, the **random forest** has the lower out-of-sample error rate of only **0.58%**. The highest was classificaiton tree with 49.96%. Stochastic gradient boosting had an out-of-sample error rate of that was close to RF with 3.7%.


## Prediction on testing data

```{r prediction_test_data}
# Now, predict the test set using the random forest since it shows the highest
# accuracy.
predict(fit_rf, ts)
```


## Final thoughts

I chose classification predictive models because the outcome variable is **categorical** (qualitative). Classification tree (rpart), stochastic gradient boosting (gbm), and random forest (rf) were among the popular choices of algorithms. I chose them because I expected that rpart would differ greatly from both gbm and rf, as stated in the lecture and from the reading materials. Out-of-sample errors computed using the validation subset (tr.validation) showed that **RF** had the highest accuracy. GBM was a fraction of a percentage point closer to rf. These results were consistent when the three models were used to predict the *classe* values for the testing set. 




